# convert the single event to a dataframe that reps the initial rowData for as many occurrences that happen
output <- rowData %>% slice(rep(1, occurrenceCount))
# update the output df with the correct occurrences and eventDatetimes
for(i in 1:nrow(output)) output$eventDatetime[i] <- as_datetime(output$datetimeStart[i] + days((i-1)*interval))
output$eventDatetime <- as_datetime(output$eventDatetime)
# remove EXDATEs; first convert the comma-separated string to a vector and datetime class
if(is.na(rowData$exdate)==FALSE) {
exdates <- as_datetime(as.vector(strsplit(rowData$exdate, ",")[[1]]))
output <- output[-which(output$eventDatetime %in% exdates),]
}
# assign occurrence
output$occurrence <- seq_along(output$occurrence)
# there may be cases where the exclusions reduce the output to 0 rows so test that before assigning a pass
if(nrow(output)>0) output$processPass <- TRUE
}
# case for weekly occurrences with variable intervals and exdate(s)
if(rowData$recurring==TRUE & rowData$freq=="WEEKLY" & is.na(rowData$interval)==FALSE & is.na(rowData$byday)==TRUE) {
interval <- rowData$interval
span <- interval(as_date(rowData$datetimeStart), latestCheckinDate)
span <- as.numeric(span, "days")
# convert span to occurrences by dividing by the interval (will be equal if interval is 1).  Use floor to rounddown
occurrenceCount <- floor(span/interval/7)+1 #7 for days in the week; +1 to ensure BYDAY partial weeks make it in
# convert the single event to a dataframe that reps the initial rowData for as many occurrences that happen
output <- rowData %>% slice(rep(1, occurrenceCount))
# update the output df with the correct occurrences and eventDatetimes
for(i in 1:nrow(output)) output$eventDatetime[i] <- as_datetime(output$datetimeStart[i] + weeks((i-1)*interval))
output$eventDatetime <- as_datetime(output$eventDatetime)
# remove EXDATEs; first convert the comma-separated string to a vector and datetime class
if(is.na(rowData$exdate)==FALSE) {
exdates <- as_datetime(as.vector(strsplit(rowData$exdate, ",")[[1]]))
output <- output[-which(output$eventDatetime %in% exdates),]
}
# assign occurrence
output$occurrence <- seq_along(output$occurrence)
# there may be cases where the exclusions reduce the output to 0 rows so test that before assigning a pass
if(nrow(output)>0) output$processPass <- TRUE
}
# case for WEEKLY BYDAY
# general strategy here is to create events for all days, then subset down to the days provided
if(rowData$recurring==TRUE & rowData$freq=="WEEKLY" & is.na(rowData$interval)==FALSE & is.na(rowData$byday)==FALSE) {
interval <- rowData$interval
span <- interval(as_date(rowData$datetimeStart), latestCheckinDate)
span <- as.numeric(span, "days")
# unlike the other cases, occurrenceCount should start off equal to the span with the strategy being to remove
# uncessessary days
occurrenceCount <- span
# convert the single event to a dataframe that reps the initial rowData for as many occurrences that happen
output <- rowData %>% slice(rep(1, occurrenceCount))
# update the output df with the correct occurrences and eventDatetimes
for(i in 1:nrow(output)) output$eventDatetime[i] <- as_datetime(output$datetimeStart[i] + days((i-1)*interval))
output$eventDatetime <- as_datetime(output$eventDatetime)
# remove events that are not provided by BYDAY
# create vector of 1-7 vals for days provided by BYDAY
includeDays <- as.vector(strsplit(rowData$byday, ",")[[1]])
includeDays[which(includeDays=="MO")] <- 1
includeDays[which(includeDays=="TU")] <- 2
includeDays[which(includeDays=="WE")] <- 3
includeDays[which(includeDays=="TH")] <- 4
includeDays[which(includeDays=="FR")] <- 5
includeDays[which(includeDays=="SA")] <- 6
includeDays[which(includeDays=="SU")] <- 7
includeDays <- as.integer(includeDays)
output <- output[which(wday(output$eventDatetime, week_start = 1) %in% includeDays),]
# remove events that don't happen on the specified weekly interval by calculating the weeks from the initial
# datetimestart and testing if the week is divisible by the interval
weekIntervals <- as.numeric(interval(as_date(rowData$datetimeStart), output$eventDatetime), "weeks")
# use ceiling such that the 6 days after the datetimestart get rouned to 1 (being the first week) and so on
weekIntervals <- ceiling(weekIntervals)
# test modulus == 0 for weekintervals (these are the events that happen at the specified interval)
weekIntervalPass <- rowData$interval %% weekIntervals
#subset to remove events which don't have a 0 modulo
output <- output[which(weekIntervalPass==0),]
# remove EXDATEs; first convert the comma-separated string to a vector and datetime class
if(is.na(rowData$exdate)==FALSE) {
exdates <- as_datetime(as.vector(strsplit(rowData$exdate, ",")[[1]]))
output <- output[-which(output$eventDatetime %in% exdates),]
}
# assign occurrence
output$occurrence <- seq_along(output$occurrence)
# there may be cases where the exclusions reduce the output to 0 rows so test that before assigning a pass
if(nrow(output)>0) output$processPass <- TRUE
}
# ensure no future events make it through for those that were processed
if(is.na(output$eventDatetime[1])==FALSE) output <- output[which(output$eventDatetime <= latestCheckinDate),]
# remove events that happen after an UNTIL date
if(is.na(rowData$until)==FALSE) output <- output[which(output$eventDatetime < rowData$until),]
output
}
# empty df to hold the results of routineExpander
calendarEventsRaw <- app_routines[FALSE,]
# add the missing columns to make it match output
calendarEventsRaw$eventDatetime <- ymd_hms()
calendarEventsRaw$processPass <- logical()
## Loop which applies routineExpander to app_routines and combines each row's output
for(i in 1:nrow(app_routines)){
processedRowData <- routineExpander(i)
calendarEventsRaw <- bind_rows(calendarEventsRaw,processedRowData)
}
# to avoid needing to rerun the routineExpander, advance to a non-raw name condition
calendarEvents <- calendarEventsRaw
# convert from datetime to date to align with the class needed to join on in app_checkins
calendarEvents$eventDate <- as_date(calendarEvents$eventDatetime)
# pull in the checkin and routine category data, then mutate a checkinFlag
calendarEvents <- left_join(calendarEvents, app_checkins, by = c("eventDate" = "date", "id" = "routineId")) %>%
left_join(app_categories, by = c("categoryId" = "id")) %>%
mutate(checkinFlag = ifelse(is.na(id.y),FALSE,TRUE)) #since id.y is the from app_checkins, is NA if not checked in
## clean the names and columns of the calendarEvents table and add in group_id/group_name
calendarEvents <- calendarEvents %>%
select(routineId = id,
routineName = name.x,
routineClass = type,
routineCat = name.y,
routineSubcat = group,
checkinFlag,
occurrence,
eventDate,
eventDatetime,
userId = userId.x,
locationId,
duration) %>%
left_join(dash_patients[,c(3,4,5)], by = c("userId" = "uuid")) %>%
left_join(dash_groups[,c(3,5,6)], by = c("group_id" = "id"))
# subtset to real patients by removing users in the test groups or with no group
patientCalendarEvents <- calendarEvents[-which(calendarEvents$group_id %in% 1:6 | is.na(calendarEvents$group_id)),]
cleanDataTime <- Sys.time()
##### Summary Results (through this script chunk, there should be no errors and the console should provide this feedback)
totalRuntime <- seconds(round(cleanDataTime - startTime,1))
processingSuccessVolume <- sum(calendarEventsRaw$processPass)
processingFailureVolume <- nrow(calendarEventsRaw[which(calendarEventsRaw$processPass==FALSE),])
daysOfPatientData <- as.numeric(interval(min(patientCalendarEvents$eventDate),max(patientCalendarEvents$eventDate)),"days")
uniquePatientVolume <- length(unique(uusers$user_id))
totalActivitiesVolume <- nrow(patientCalendarEvents)
avgActivitiesPerPatient <- totalActivitiesVolume/uniquePatientVolume
totalCheckinRate <- sum(patientCalendarEvents$checkinFlag)/nrow(patientCalendarEvents)
supportVolume <- nrow(patientCalendarEvents[which(patientCalendarEvents$routineClass=="Support"),])
supportCheckinRate <- nrow(patientCalendarEvents[which(patientCalendarEvents$checkinFlag==TRUE &
patientCalendarEvents$routineClass=="Support"),])/supportVolume
selfcareVolume <- nrow(patientCalendarEvents[which(patientCalendarEvents$routineClass=="SelfCare"),])
selfcareCheckinRate <- nrow(patientCalendarEvents[which(patientCalendarEvents$checkinFlag==TRUE &
patientCalendarEvents$routineClass=="SelfCare"),])/selfcareVolume
percentSupport <- supportVolume/totalActivitiesVolume
percentSelfcare <- selfcareVolume/totalActivitiesVolume
cat(
paste0("Summary Results:", "\n",
"Total runtime: ", totalRuntime, "\n",
"Processing success volume: ", processingSuccessVolume, "\n",
"Processing failure volume: ", processingFailureVolume, "\n",
"Total days of patient data: ", daysOfPatientData, "\n",
"Unique patient volume: ",uniquePatientVolume, "\n",
"Total activities volume: ", totalActivitiesVolume, "\n",
"Average activities per patient: ", avgActivitiesPerPatient, "\n",
"Total check-in rate: ", percent(totalCheckinRate), "\n",
"Support activities volume: ", supportVolume, "\n",
"Support check-in rate: ", percent(supportCheckinRate), "\n",
"Self-care activities volume: ", selfcareVolume, "\n",
"Self-care check-in rate: ", percent(selfcareCheckinRate), "\n",
"Percent Support activities: ", percent(percentSupport), "\n",
"Percent Self-care activities: ", percent(percentSelfcare), "\n"
)
)
app_timed_challenge_start_dates <- dbGetQuery(appCon,"SELECT * FROM timed_challenge_start_dates")
app_challenge_completions <- dbGetQuery(appCon,"SELECT * FROM challenge_completions")
app_milestones <- dbGetQuery(appCon,"SELECT * FROM milestones")
dash_one_time_use_codes <- dbGetQuery(dashCon,"SELECT * FROM one_time_use_codes")
# Use the whiteboard_maxPotentialLiabilities (pic of whiteboard brainstorm) to see the thought process
# general idea is that we build two reference tables (one for challenges and the other for milestones).  Then, using
# that plus the app_challenge_completions, we build an all-up view of each user's potential per day, then summarize
# into another table (grouping on date) to get the max expected liability per day
## Build challenge table
challengeRef <- data.frame("challengeId" = 1:22,
"maxRewardAmt" = c(rep(10,7), rep(15,3), rep(20,3), rep(25,4), rep(30,5)),
"challengeLength" = c(NA,NA,NA,3,4,5,6,7,8,9,11,13,15,17,19,21,24,27,30,33,36,39),
"timerDays" = c(NA,NA,NA,1,4,9,15,22,30,39,50,63,78,95,114,135,159,186,216,249,285,324))
## Build milestone table
milestoneRef <- data.frame("milestoneId" = 1:6,
"maxMilestoneAmt" = c(rep(50,4),75,100),
"timerDays" = c(30,60,90,183,274,365))
## Build the comprehensive table
# start by generating a vector of all potential rewards users (excluding unredeemed codes)
rewardsUsers <- dash_one_time_use_codes$patient_uuid[which(is.na(dash_one_time_use_codes$patient_uuid)==FALSE)]
# note: milestone and challenge 4 timer starts on the midnight of the day challenge 3 completes (for the timezone
# of the patient)
# Because challenges 1-3 can be completed same-day and we're assuming max liability, for all users who have not
# yet comopleted those challenges, they should be assumed to be doing those same-day.
# vector of rewardsUsers who have started their timers
timedUsers <- app_timed_challenge_start_dates$userId
# vector of rewardsUsers who have not started their timers (and for which we'll assume they can complete 3 today)
untimedUsers <- rewardsUsers[-which(rewardsUsers %in% timedUsers)]
# build the table of max liability expectations for the untimed users; keep in mind that even untimed users
# could have completed challenges 1 and 2 without being timed so the methodology will be to build the same schedule for
# all users (assuming completing through 3 today), then remove/update for rewards already paid out
## Build complete df of all challenges and rewards to be updated per user with their status and forecast
rewardsTemplate <- data.frame("completeDate" = as_date(rep(NA,28)), # 28 rewards
"userId" = as.integer(rep(NA,28)),
"rewardType" = c(rep("challenge",22), rep("milestone",6)), # milestone or challenge
"rewardId" = as.integer(rep(NA,28)), # this will be from the 'id' field in milestones and challenge_completions
"rewardNumber" = as.integer(c(1:22,1:6)), # challenge 1-22, milestone 1-6
"maxRewardAmt" = as.integer(c(challengeRef$maxRewardAmt,milestoneRef$maxMilestoneAmt)),
"rewardEarned" = as.integer(rep(NA,28)),
"timerDays" = as.integer(c(challengeRef$timerDays,milestoneRef$timerDays)),# days since timed_challenge_start_dates, NA if no timer
"realizedFactor" = factor(rep(NA,28), levels = c(NA, "historical", "forecasted")))
## Build a per-user optimistic schedule of rewards
# function which accepts userId and outputs the max optimistic view of their engagement. i.e. they finish challenges
# 1-4 same day.  This should also include all their future challenge completions (assuming each will be completed on
# the first day) and milestones (assuming they earn all of them)
userForecaster <- function(userId) {
#### Build output
# Generally, the flow is to fill in historical data to the template, then forecast the future
### Start with the template
output <- rewardsTemplate
# plug in userId
output$userId <- as.integer(rep(userId,28))
### Extract historical data
## Challenges
# TODO: figure out how the timezone issue affects the timer's start
# replace NAs for rewardId with the id in the app_challenge_completions table
# not sure how to make this more readable... apologies--was a real puzzle to build
output$rewardId[which(output$rewardNumber %in%
app_challenge_completions$challengeId[which(app_challenge_completions$userId==userId)] &
output$rewardType=="challenge")] <-
app_challenge_completions$id[which(app_challenge_completions$userId==userId)]
# plug in complete date and reward earned based on the id extracted
output$completeDate[which(is.na(output$rewardId)==FALSE)] <-
as_date(app_challenge_completions$createdAt[which(app_challenge_completions$id %in% output$rewardId)])
output$rewardEarned[which(is.na(output$rewardId)==FALSE)] <-
app_challenge_completions$earned[which(app_challenge_completions$id %in% output$rewardId)]
## Milestones
output$rewardId[which(output$rewardNumber %in%
app_milestones$milestoneId[which(app_milestones$userId==userId)] &
output$rewardType=="milestone")] <-
app_milestones$id[which(app_milestones$userId==userId)]
# plug in complete date and reward earned based on the id extracted
output$completeDate[which(is.na(output$rewardId)==FALSE & output$rewardType=="milestone")] <-
as_date(app_milestones$createdAt[which(app_milestones$id %in% output$rewardId[which(output$rewardType=="milestone")])])
output$rewardEarned[which(is.na(output$rewardId)==FALSE & output$rewardType=="milestone")] <-
app_milestones$earned[which(app_milestones$id %in% output$rewardId[which(output$rewardType=="milestone")])]
### Forecast future
## Update realized state and assumptions about earnings
# updated realizedFactor based on results from historical data
output$realizedFactor[which(is.na(output$rewardId)==FALSE)] <- "historical"
output$realizedFactor[which(is.na(output$realizedFactor)==TRUE)] <- "forecasted"
# under the optimistic assumption, all rewards are earned. Thus, 'earned' should equal maxRewardAmt
output$rewardEarned[which(output$realizedFactor=="forecasted")] <-
output$maxRewardAmt[which(output$realizedFactor=="forecasted")]
## Set the timer start for the schedule of rewards
# If they haven't finished challenge 3, then assume tomorrow (i.e. assume they'll do through 3 today) else assign
# based on their app_timed_challenge_start_dates$start_date
if(userId %in% app_timed_challenge_start_dates$userId) {
startDate <- app_timed_challenge_start_dates$startDate[which(app_timed_challenge_start_dates$userId==userId)]
}
if((userId %in% app_timed_challenge_start_dates$userId)==FALSE) startDate <- as_date(Sys.Date())
# Loop to update expected completions, but only for rows where there isn't historical data
for(i in 1:nrow(output)){
# set expectation for completing challenges 1-4 if not set historically
if(is.na(output$completeDate[i]) & is.na(output$timerDays[i])){
output$completeDate[i] <- startDate
}
# set expectation for challanges not in the first 4 and milestones
if(is.na(output$completeDate[i])){
output$completeDate[i] <- startDate + days(output$timerDays[i])
}
}
output
}
## Create a df by looping through all users and binding together
# prime the pump with an empty df in the right format
rewardsForecast <- rewardsTemplate[FALSE,]
for(i in 1:length(rewardsUsers)){
userForecast <- userForecaster(i)
rewardsForecast <- bind_rows(rewardsForecast,userForecast)
}
## Group into a summary tab per day
rewardsForecastTab <- rewardsForecast %>%
group_by(completeDate) %>%
summarize(maxLiability = sum(rewardEarned)) %>%
mutate(cumeLiability = cumsum(maxLiability))
# export to csv
write.csv(rewardsForecast, "weconnect_rewardsForecast_details_20190109.csv", row.names = FALSE)
write.csv(rewardsForecastTab, "weconnect_rewardsForecast_summary_20190109.csv", row.names = FALSE)
getwd()
View(rewardsForecastTab)
View(rewardsForecast)
View(app_challenge_completions)
View(app_users)
View(app_user_profiles)
View(milestoneRef)
View(challengeRef)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
########################################################################################################################
# VERSION NOTES
########################################################################################################################
# v1: got everything up and running to produce a functioning report
# v2: I want to do most of the experimentation in a .R file due to how much easier it is to work in that environment.
# to that end, I'll do all my work in the 'WEconnect_reporting_v[number].R' file and then specify the details
# regarding which code chunks were brought over in this section.  The main motivation is to make it so I primarily
# use this .Rmd file to figure out the presentation, not the ETL.
# another thing I want to sort out is to have the report adjust based on whether its a single group or if it has
# multiple groups in one org.  Also, different clients treat groups as case managers so I'll want to accomodate.
# v3: created since v2 is stable and I want to experiment with the YAML metadata block
#   # moved the title and date of the YAML metadata block to the bottom (TIL I can!); now it'll be automatic
# v4: I noticed that i had two versions of v3 and this seemed to be the more updated once since it fixed the YAML header
#   # and footer so I just named this v4
#   # fixed an issue where the activitiesCompositioner breaks if the patient only did activities of one class
#-----General Notes------
# the cat() function is very sensitive to being at the left-most part of the editor so make sure to never indent it
########################################################################################################################
# Load Packages
########################################################################################################################
library(dplyr)
library(tidyr)
library(ggplot2)
library(treemapify)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
########################################################################################################################
# VERSION NOTES
########################################################################################################################
# v1: got everything up and running to produce a functioning report
# v2: I want to do most of the experimentation in a .R file due to how much easier it is to work in that environment.
# to that end, I'll do all my work in the 'WEconnect_reporting_v[number].R' file and then specify the details
# regarding which code chunks were brought over in this section.  The main motivation is to make it so I primarily
# use this .Rmd file to figure out the presentation, not the ETL.
# another thing I want to sort out is to have the report adjust based on whether its a single group or if it has
# multiple groups in one org.  Also, different clients treat groups as case managers so I'll want to accomodate.
# v3: created since v2 is stable and I want to experiment with the YAML metadata block
#   # moved the title and date of the YAML metadata block to the bottom (TIL I can!); now it'll be automatic
# v4: I noticed that i had two versions of v3 and this seemed to be the more updated once since it fixed the YAML header
#   # and footer so I just named this v4
#   # fixed an issue where the activitiesCompositioner breaks if the patient only did activities of one class
#-----General Notes------
# the cat() function is very sensitive to being at the left-most part of the editor so make sure to never indent it
########################################################################################################################
# Load Packages
########################################################################################################################
library(dplyr)
library(tidyr)
library(ggplot2)
library(RPostgreSQL)
library(knitr)
library(markdown)
library(rmarkdown)
library(gridExtra)
library(scales)
library(kableExtra)
library(data.table)
pw<-{
"p7f9a863507e52fce9f8aad03bdf32f059fcd65d22ef61a415355c98e43f2c2c4"
}
# creates a connection to the postgres database; notice that we're not using dbDriver("PostgreSQL") as first arg
con <- dbConnect(RPostgres::Postgres(),
dbname = "d1gonk7v94qntr",
host = "ec2-35-170-121-65.compute-1.amazonaws.com",
port = 5432,
user = "u4cbdr5olshn6d",
password = pw)
rm(pw) # removes the password
groups <- dbGetQuery(con,"SELECT * FROM groups")
organizations <- dbGetQuery(con,"SELECT * FROM organizations")
subsetter <- function(group, compareOrg=FALSE) {
groupPicker <<- group
# if the user wants to compareOrg, then all the groups associated with the organization of the one group are put in a vector
if(compareOrg==TRUE) {
groupPicker <<- unique(groups$group_id[which(groups$organization_id==groups$organization_id[which(groups$group_id==group)])])
}
# put groups in groupPicker into a string for the SQL query
group_ids <<- character(0)
for(i in 1:length(groupPicker)){
group_ids <<- paste0(group_ids, groupPicker[i],",")
}
#add parenthesis before and after and remove trailing comma
group_ids <<- substr(group_ids,1,nchar(group_ids)-1)
group_ids <<- paste0("(",group_ids,")")
# create client name for reporting and don't print
invisible(
ifelse(length(groupPicker)>1,
client <<- organizations$name[which(organizations$id==groups$organization_id[which(groups$group_id==group)])],
client <<- groups$group_name[which(groups$group_id==group)]
)
)
}
# 69 is VOALA (second arg is TRUE due to multiple facilities)
# 47 is City Team (second arg is TRUE)
# 48 is Assurance Recovery Monitoring (second arg is FALSE because it only has one facility)
group <- 69
subsetter(group,TRUE)
group_ids
data <- dbGetQuery(con, paste0(
"SELECT
activities.routine_name,
calendarevents.date_start,
calendarevents.did_checkin,
users.user_id,
users.first_name,
users.last_name,
users.soberday,
groups.group_id,
groups.group_name,
calendarevents.duration
FROM activities
JOIN calendarevents
ON activities.activity_id = calendarevents.activity_id
JOIN users
ON activities.user_id = users.user_id
FULL JOIN groups
ON users.group_id = groups.group_id
WHERE
users.tester = FALSE
AND groups.group_id IN ",group_ids)
)
View(data)
test <- data %>% group_by(user_id) %>% summarize(first_name = first(first_name), last_name = first(last_name))
View(test)
library(kableExtra)
?kableExtra
install.packages("kableExtra")
# LOAD PACKAGES
library(ggplot2)
library(dplyr)
library(scales)
library(ggthemes)
library(qqplotr)
library(gridExtra)
library(latex2exp)
install.packages("latex2exp")
library(latex2exp)
distribution_visualizer <- function(
data,
title = "Histogram and Density",
x = "Values",
binwidthInput = (max(data)-min(data))/15){
binwidthInput <- binwidthInput
binCounts <- .bincode(data, seq(0,max(data), binwidthInput))
xbar <- round(mean(data),1)
sd <- round(sd(data),1)
g <- ggplot(data.frame(data), aes(data)) +
geom_histogram(fill = colors[1],
color = colors[2],
binwidth = binwidthInput) +
geom_vline(aes(xintercept = mean(data)),
color = colors[3],
linetype = "dashed",
size = 0.7) +
geom_density(aes(y = binwidthInput * ..count..),
alpha = 0.2,
fill = colors[2],
color = colors[1]) +
labs(title = title,
x = x,
y = "Frequency") +
annotate("text",
x = mean(data)*1.25,
y = max(binCounts, na.rm = TRUE)*0.75,
label = paste0("Mean = ", xbar),
size = 3) +
theme_bw()
p <- ggplot(data.frame(data), aes(sample = data)) +
stat_qq_band(color = colors[1], fill = colors[2]) +
stat_qq_line(color = colors[3], linetype = "dashed", size = 0.7) +
stat_qq_point(size = 0.8, alpha = 0.3) +
labs(title = "IQ Q-Q Plot",
x = "Theoretical Values",
y = "Sample Values") +
theme_bw()
grid.arrange(g, p, ncol = 2)
}
distribution_visualizer(mtcars$mpg)
# set up color palettes
colors <- ggthemes_data[["tableau"]][["color-palettes"]][["regular"]][[2]][[2]]
distribution_visualizer(mtcars$mpg)
distribution_visualizer <- function(
data,
title = "Histogram and Density",
x = "Values",
binwidthInput = (max(data)-min(data))/15){
binwidthInput <- binwidthInput
binCounts <- .bincode(data, seq(0,max(data), binwidthInput))
xbar <- round(mean(data),1)
sd <- round(sd(data),1)
g <- ggplot(data.frame(data), aes(data)) +
geom_histogram(fill = colors[1],
color = colors[2],
binwidth = binwidthInput) +
geom_vline(aes(xintercept = mean(data)),
color = colors[3],
linetype = "dashed",
size = 0.7) +
geom_density(aes(y = binwidthInput * ..count..),
alpha = 0.2,
fill = colors[2],
color = colors[1]) +
labs(title = title,
x = x,
y = "Frequency") +
annotate("text",
x = mean(data)*1.25,
y = max(binCounts, na.rm = TRUE)*0.75,
label = paste0("\\u03BC = ", xbar),
size = 3) +
theme_bw()
p <- ggplot(data.frame(data), aes(sample = data)) +
stat_qq_band(color = colors[1], fill = colors[2]) +
stat_qq_line(color = colors[3], linetype = "dashed", size = 0.7) +
stat_qq_point(size = 0.8, alpha = 0.3) +
labs(title = "IQ Q-Q Plot",
x = "Theoretical Values",
y = "Sample Values") +
theme_bw()
grid.arrange(g, p, ncol = 2)
}
distribution_visualizer(mtcars$mpg)
getwd()
setwd("C:/Users/Will/Documents/UW/DATA557/WEEK02")
getwd()
