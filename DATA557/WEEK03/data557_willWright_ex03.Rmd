---
title: "DATA557 Exercise 3, Problem 3"
author: "Tara Wilson, Lauren Heintz, Ben Bordeur Mathieu, and Will Wright"
date: "January 23, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# LOAD PACKAGES
library(ggplot2)
library(dplyr)
library(scales)
library(ggthemes)
library(qqplotr)
library(gridExtra)
library(BSDA)

# read file
pDat <- read.csv("../WEEK03/process.csv")

```


Question 3 

For this question you are to work in groups. Find a group of 3 or 4 students and work together to come to a solution. One member of the group will report the results verbally for the group at the end of the exercise. The reporter also posts the solution to the dropbox for Exercise 3. Include the names of the members of the group on the solution.

Suppose that a new experiment is being designed to determine the effect on output of temperatures higher than 100. In particular, the aim of the new experiment is to test the null hypothesis that the mean output is the same for temperature 100 and temperature 120. The researcher would like to have at least 90% power to detect a difference between these conditions in mean output equal to 75. Your job is to determine the sample sizes for each group and to decide which test statistic will be used to test the null hypothesis. Justify your answers.

&nbsp;  

```{r question3}
set.seed(999)

# calculate input parameters
output_diff <- 75
sd_100 <- sd(pDat$output[which(pDat$temp==100)])
n_vals <- 1:100
reps=1000

# simulations
powers_z <- rep(NA,length(n_vals))
powers_t <- rep(NA,length(n_vals))
powers_w <- rep(NA,length(n_vals))
for(j in 1:length(n_vals)){
  test_statistic <- rep(NA,reps)
  for(i in 1:reps){
    x <- rnorm(n_vals[j], output_diff, sd_100)
    y <- rnorm(n_vals[j], output_diff, sd_100)
    se <- sqrt(var(x)/n_vals[j]+var(y)/n_vals[j])
    test_statistic[i] <- (output_diff)/se
  }
  powers_z[j] <- mean(abs(test_statistic)>qnorm(0.975)) # for z-test
  # powers_t[j] <- mean(abs(test_statistic)>qt(0.975, df = )) # incomplete for t-test
  # powers_w[j] <- mean(abs(test_statistic)>qt(0.975)) # incomplete for welch's t-test
}
results <- data.frame(n = n_vals, power = powers_z)

g <- ggplot(results, aes(x = n, y = power))
g + geom_line() + 
  scale_x_continuous(limits = c(10,50)) +
  theme_bw() +
  geom_hline(yintercept = 0.9, col = "red", linetype = "dashed")

ideal_n <- max(results$n[which(results$power<=0.9)]) # using z-test; though this is not our recommendation

```

The results of this power analysis show that the needed sample size is `r ideal_n` assuming we wanted to use the z-statistic to calculate power.  

We plan to conduct a simulation study to find a sample size suited for a power of 0.9 using the Welch t-test so that our test does not rely on equal variances. Additionally, this allows our sample sizes to vary if any errors occur in sampling as well.

Then we would conduct a simulation study to assess the performance of the Welch t-test using simulated data from t distributions using the sample mean and variance of the 100 degrees data. We could also introduce variations to our sample sizes and variances to test the resilience of our experiment structure. The reliance would be evaluated by getting a balance between type one error probably (about 0.05) while maintaining the power above 0.9.